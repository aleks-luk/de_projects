# de_projects
This project demonstrates building a data pipeline that extracts, transforms, and loads (ETL) data from various sources into a relational database. The pipeline involves scraping book data, fetching user data from an API, transforming the raw data, uploading the transformed data to an AWS S3 bucket, and finally loading it into an AWS RDS MySQL database.
Technologies used: SQL, Python, AWS, Bash.
Next steps: Refactor code to improve maintainability and performance, and submit for code review to ensure quality and adherence to best practices, and migrate the data structure to Snowflake for better scalability and performance.
